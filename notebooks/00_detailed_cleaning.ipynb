{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "2999b498",
      "cell_type": "markdown",
      "source": "\n# 00 \u2014 Detailed Data Cleaning (Amazon Electronics Reviews)\n\nThis notebook performs **thorough data cleaning** on the Amazon Electronics reviews dataset\n(e.g., `Electronics_5.json.gz`) **before any modeling**.\n\n**Expected input columns** (matching your dataset):\n```\n['overall','vote','verified','reviewTime','reviewerID','asin','style',\n 'reviewerName','reviewText','summary','unixReviewTime','image']\n```\n\n### What this notebook does\n- Streams the input JSON-lines (gz OK) to be **memory safe**.\n- Drops the **`image`** column (not used).\n- Normalizes: whitespace, control chars, unescape HTML, **mask PII** (URLs, emails, phone numbers).\n- Converts `vote` \u2192 **int**, `verified` \u2192 **bool**.\n- Parses time (`unixReviewTime` preferred; falls back to `reviewTime`) \u2192 **unix_time** + **ISO datetime**.\n- Detects **language**; keeps **English**, logs counts.\n- Filters **too-short**/**nonsensical** reviews.\n- **Deduplicates** on `(reviewerID, asin, unix_time, reviewText)`.\n- Flattens `style` into `style_color`, `style_size`, `style_other`.\n- Writes:\n  - `data/cleaned_reviews.csv`\n  - `data/removed_reviews.csv`\n  - `data/language_stats.json`\n- Quick EDA with **matplotlib** (one chart per figure; no seaborn; default colors).\n\n> Adjust the **paths & limits** in the next cell as needed.\n",
      "metadata": {}
    },
    {
      "id": "6c27aae1",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# === Configuration ===\nfrom pathlib import Path\n\n# Change this if your project root is different\nPROJECT_ROOT = Path.cwd()\n\n# Input dataset path (JSON-lines; gz is fine)\nRAW_PATH = PROJECT_ROOT / \"data\" / \"Electronics_5.json.gz\"\n\n# Output paths\nCLEANED = PROJECT_ROOT / \"data\" / \"cleaned_reviews.csv\"\nREMOVED = PROJECT_ROOT / \"data\" / \"removed_reviews.csv\"\nLANGJSON = PROJECT_ROOT / \"data\" / \"language_stats.json\"\n\n# Processing thresholds\nMIN_TEXT_LEN = 20        # drop reviews shorter than this (after cleaning)\nKEEP_LANGS = {\"en\"}      # keep English reviews only\n\n# Streaming write chunk size (rows per flush)\nWRITE_CHUNK_SIZE = 50000  # lower if RAM is tight\n\n# For quick test runs on laptop, set a positive limit (e.g., 200_000). 0 = no limit\nMAX_ROWS = 0\n\nprint(\"Input:\", RAW_PATH)\nprint(\"Outputs:\", CLEANED, REMOVED, LANGJSON)\n",
      "outputs": []
    },
    {
      "id": "c3e1d954",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# === Imports ===\nimport os, re, json, html, math, gzip, io, hashlib, ast, datetime as dt\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 42\n\n# Ensure output dirs exist\nCLEANED.parent.mkdir(parents=True, exist_ok=True)\nREMOVED.parent.mkdir(parents=True, exist_ok=True)\nLANGJSON.parent.mkdir(parents=True, exist_ok=True)\n\nassert RAW_PATH.exists(), f\"Input file not found: {RAW_PATH}\\nPlace your Electronics_5.json.gz there.\"\nprint(\"\u2705 Environment ready.\")\n",
      "outputs": []
    },
    {
      "id": "2f05233a",
      "cell_type": "markdown",
      "source": "### Helper functions: streaming, cleaning, parsing, PII masking, style flattening",
      "metadata": {}
    },
    {
      "id": "91421a81",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# --- Streaming JSON-lines reader (handles .gz) ---\ndef _open_text(path: Path):\n    if str(path).endswith(\".gz\"):\n        return io.TextIOWrapper(gzip.open(path, \"rb\"), encoding=\"utf-8\", errors=\"ignore\")\n    return open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\")\n\ndef stream_jsonl(path: Path):\n    with _open_text(path) as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                yield json.loads(line)\n            except json.JSONDecodeError:\n                # Skip malformed line\n                continue\n\n# --- ID and parsing helpers ---\ndef stable_id(*parts: str) -> str:\n    return hashlib.md5((\"||\".join(parts)).encode(\"utf-8\")).hexdigest()\n\nWS_RE = re.compile(r\"\\s+\")\nCTRL_RE = re.compile(r\"[\\x00-\\x1f\\x7f]\")\nURL_RE = re.compile(r\"(https?://\\S+)\", re.I)\nEMAIL_RE = re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[A-Za-z]{2,}\\b\")\nPHONE_RE = re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?){1,2}\\d{4}\\b\")\n\ndef normalize_ws(txt: str) -> str:\n    return WS_RE.sub(\" \", (txt or \"\").strip())\n\ndef strip_ctrl(txt: str) -> str:\n    return CTRL_RE.sub(\" \", txt)\n\ndef strip_html_entities(txt: str) -> str:\n    return html.unescape(txt)\n\ndef mask_pii(txt: str) -> str:\n    txt = URL_RE.sub(\"[URL]\", txt)\n    txt = EMAIL_RE.sub(\"[EMAIL]\", txt)\n    txt = PHONE_RE.sub(\"[PHONE]\", txt)\n    return txt\n\ndef clean_text(txt: str) -> str:\n    txt = str(txt or \"\")\n    txt = strip_html_entities(txt)\n    txt = strip_ctrl(txt)\n    txt = normalize_ws(txt)\n    txt = mask_pii(txt)\n    return txt\n\ndef to_int_votes(v) -> int:\n    if v is None or (isinstance(v, float) and math.isnan(v)):\n        return 0\n    try:\n        return int(str(v).replace(\",\", \"\").strip())\n    except Exception:\n        return 0\n\ndef to_bool_verified(v) -> bool:\n    if isinstance(v, bool):\n        return v\n    if v is None:\n        return False\n    s = str(v).strip().lower()\n    if s in (\"true\",\"t\",\"1\",\"yes\",\"y\"):\n        return True\n    if s in (\"false\",\"f\",\"0\",\"no\",\"n\"):\n        return False\n    return False\n\ndef parse_review_time(rtime, unix_time):\n    # Prefer unixReviewTime if present; else parse reviewTime\n    if unix_time is not None and not (isinstance(unix_time, float) and math.isnan(unix_time)):\n        try:\n            ut = int(unix_time)\n            dt_iso = dt.datetime.utcfromtimestamp(ut).isoformat() + \"Z\"\n            return ut, dt_iso\n        except Exception:\n            pass\n    if rtime:\n        try:\n            dtobj = pd.to_datetime(rtime, errors=\"coerce\")\n            if pd.isna(dtobj):\n                return None, None\n            ut = int(dtobj.timestamp())\n            return ut, dtobj.isoformat() + \"Z\"\n        except Exception:\n            return None, None\n    return None, None\n\ndef detect_lang_safe(txt: str) -> str:\n    try:\n        return detect(txt) if len(txt) >= 20 else \"en\"\n    except Exception:\n        return \"unknown\"\n\ndef is_nonsensical(txt: str) -> bool:\n    t = txt.replace(\" \", \"\")\n    if len(t) >= 30:\n        uniq_ratio = len(set(t)) / max(1, len(t))\n        if uniq_ratio < 0.1:\n            return True\n    return False\n\ndef parse_style(style_field):\n    \"\"\"Extract color/size-like fields from 'style' column.\n    Returns: (style_raw, style_color, style_size, style_other)\n    \"\"\"\n    raw = None\n    d = {}\n    color = None\n    size = None\n    other = None\n\n    if style_field is None or (isinstance(style_field, float) and math.isnan(style_field)):\n        return \"\", \"\", \"\", \"\"\n\n    if isinstance(style_field, dict):\n        raw = json.dumps(style_field, ensure_ascii=False)\n        d = style_field\n    else:\n        s = str(style_field).strip()\n        raw = s\n        # Try parse as dict\n        try:\n            v = ast.literal_eval(s)\n            if isinstance(v, dict):\n                d = v\n        except Exception:\n            # fallback: split on ; or , as key:val\n            parts = [p.strip() for p in re.split(r\"[;,]\", s) if p.strip()]\n            for p in parts:\n                if \":\" in p:\n                    k, v = p.split(\":\", 1)\n                    d[k.strip()] = v.strip()\n\n    for k, v in d.items():\n        kl = k.lower()\n        if (\"color\" in kl or \"colour\" in kl) and color is None:\n            color = str(v)\n        if (\"size\" in kl or \"capacity\" in kl or \"storage\" in kl) and size is None:\n            size = str(v)\n\n    if d and not (color or size):\n        other = \"; \".join([f\"{k}: {v}\" for k, v in d.items()])\n\n    return raw or \"\", color or \"\", size or \"\", other or \"\"\n",
      "outputs": []
    },
    {
      "id": "1835bfcd",
      "cell_type": "markdown",
      "source": "### Cleaning loop: stream \u2192 clean \u2192 filter \u2192 dedupe \u2192 write CSV",
      "metadata": {}
    },
    {
      "id": "b987c474",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\nfrom collections import Counter\n\nCOLUMNS = [\n    \"review_id\", \"product_id\", \"user_id\",\n    \"rating\", \"verified\", \"vote\", \"vote_int\",\n    \"unix_time\", \"review_time\", \"review_time_iso\",\n    \"summary\", \"text\", \"lang\",\n    \"text_len\", \"word_count\",\n    \"style_raw\", \"style_color\", \"style_size\", \"style_other\"\n]\n\n# Initialize output CSVs with headers\npd.DataFrame(columns=COLUMNS).to_csv(CLEANED, index=False)\npd.DataFrame(columns=[\"reason\",\"reviewerID\",\"asin\"]).to_csv(REMOVED, index=False)\n\n# State\ndedupe_seen = set()\nlang_counts = Counter()\nclean_buffer = []\nremoved_all = []\nn_seen = 0\n\nfor rec in stream_jsonl(RAW_PATH):\n    n_seen += 1\n    # Extract fields\n    overall = rec.get(\"overall\", None)\n    vote = rec.get(\"vote\", None)\n    verified = rec.get(\"verified\", None)\n    reviewTime = rec.get(\"reviewTime\", None)\n    reviewerID = rec.get(\"reviewerID\", None)\n    asin = rec.get(\"asin\", None)\n    style = rec.get(\"style\", None)\n    reviewerName = rec.get(\"reviewerName\", None)  # unused downstream\n    reviewText = rec.get(\"reviewText\", rec.get(\"review\", \"\"))\n    summary = rec.get(\"summary\", \"\")\n    unixReviewTime = rec.get(\"unixReviewTime\", None)\n    # (drop 'image' entirely)\n\n    # Required keys\n    if not reviewerID or not asin:\n        removed_all.append({\"reason\": \"missing_keys\", \"reviewerID\": reviewerID, \"asin\": asin})\n        continue\n\n    # Clean text\n    txt = clean_text(reviewText)\n    summ = clean_text(summary)\n\n    # Drop short/nonsense\n    if len(txt) < MIN_TEXT_LEN or is_nonsensical(txt):\n        removed_all.append({\"reason\": \"short_or_nonsense\", \"reviewerID\": reviewerID, \"asin\": asin})\n        continue\n\n    # Language\n    lang = detect_lang_safe(txt)\n    lang_counts[lang] += 1\n    if lang not in KEEP_LANGS:\n        removed_all.append({\"reason\": f\"lang_{lang}\", \"reviewerID\": reviewerID, \"asin\": asin})\n        continue\n\n    # Rating\n    try:\n        rating = float(overall)\n    except Exception:\n        rating = None\n    if rating is None or not (1.0 <= rating <= 5.0):\n        removed_all.append({\"reason\": \"bad_rating\", \"reviewerID\": reviewerID, \"asin\": asin})\n        continue\n\n    # Time\n    ut, dt_iso = parse_review_time(reviewTime, unixReviewTime)\n    if ut is None:\n        removed_all.append({\"reason\": \"bad_time\", \"reviewerID\": reviewerID, \"asin\": asin})\n        continue\n\n    # Dedup\n    dedupe_key = (str(reviewerID), str(asin), int(ut), txt)\n    if dedupe_key in dedupe_seen:\n        removed_all.append({\"reason\": \"duplicate\", \"reviewerID\": reviewerID, \"asin\": asin})\n        continue\n    dedupe_seen.add(dedupe_key)\n\n    # Votes/verified\n    vote_int = to_int_votes(vote)\n    verified_bool = to_bool_verified(verified)\n\n    # Style normalization\n    style_raw, style_color, style_size, style_other = parse_style(style)\n\n    # Stable review_id\n    review_id = stable_id(str(reviewerID), str(asin), str(ut), txt[:64])\n\n    # Derived\n    words = txt.split()\n    word_count = len(words)\n\n    clean_buffer.append({\n        \"review_id\": review_id,\n        \"product_id\": str(asin),\n        \"user_id\": str(reviewerID),\n        \"rating\": rating,\n        \"verified\": verified_bool,\n        \"vote\": vote if vote is not None else \"\",\n        \"vote_int\": int(vote_int),\n        \"unix_time\": int(ut),\n        \"review_time\": reviewTime if reviewTime is not None else \"\",\n        \"review_time_iso\": dt_iso if dt_iso is not None else \"\",\n        \"summary\": summ,\n        \"text\": txt,\n        \"lang\": lang,\n        \"text_len\": len(txt),\n        \"word_count\": word_count,\n        \"style_raw\": style_raw,\n        \"style_color\": style_color,\n        \"style_size\": style_size,\n        \"style_other\": style_other,\n    })\n\n    # Periodic flush\n    if len(clean_buffer) >= WRITE_CHUNK_SIZE:\n        pd.DataFrame(clean_buffer, columns=COLUMNS).to_csv(CLEANED, mode=\"a\", header=False, index=False)\n        clean_buffer = []\n\n    # Progress + max rows guard\n    if n_seen % 10000 == 0:\n        print(f\"Processed {n_seen:,} lines... cleaned so far: {sum(1 for _ in open(CLEANED, 'r', encoding='utf-8'))-1:,}\")\n    if MAX_ROWS and n_seen >= MAX_ROWS:\n        print(f\"Reached MAX_ROWS={MAX_ROWS}, stopping early.\")\n        break\n\n# Final flush\nif clean_buffer:\n    pd.DataFrame(clean_buffer, columns=COLUMNS).to_csv(CLEANED, mode=\"a\", header=False, index=False)\n\n# Write removed & language stats\nif removed_all:\n    pd.DataFrame(removed_all)[[\"reason\",\"reviewerID\",\"asin\"]].to_csv(REMOVED, mode=\"a\", header=False, index=False)\n\nwith open(LANGJSON, \"w\", encoding=\"utf-8\") as f:\n    json.dump({\"counts\": dict(lang_counts)}, f, indent=2)\n\nprint(\"\u2705 Cleaning complete.\")\nprint(\"Wrote:\", CLEANED, REMOVED, LANGJSON)\n",
      "outputs": []
    },
    {
      "id": "d4696959",
      "cell_type": "markdown",
      "source": "### Quick EDA (matplotlib-only, one chart per figure)",
      "metadata": {}
    },
    {
      "id": "5fa8e14b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# Load a manageable slice for EDA\ndf = pd.read_csv(CLEANED, nrows=200000)  # adjust if needed\nprint(\"EDA sample shape:\", df.shape)\ndf.head()\n",
      "outputs": []
    },
    {
      "id": "451b3a3c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# Ratings distribution\nfig, ax = plt.subplots()\nax.hist(df['rating'].dropna(), bins=[1,2,3,4,5,6], align='left', rwidth=0.9)\nax.set_xlabel(\"Stars\"); ax.set_ylabel(\"Count\"); ax.set_title(\"Ratings distribution\")\nplt.show()\n",
      "outputs": []
    },
    {
      "id": "a1918795",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# Text length distribution (log scale)\nfig, ax = plt.subplots()\nax.hist(df['text_len'].clip(upper=2000), bins=50)\nax.set_yscale('log')\nax.set_xlabel(\"Text length (chars)\"); ax.set_ylabel(\"Count (log)\"); ax.set_title(\"Review length distribution\")\nplt.show()\n",
      "outputs": []
    },
    {
      "id": "0460ce90",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# Helpful votes vs rating\nfig, ax = plt.subplots()\nax.scatter(df['rating'], df['vote_int'], s=2, alpha=0.3)\nax.set_xlabel(\"Rating\"); ax.set_ylabel(\"Votes\"); ax.set_title(\"Helpful votes vs Rating\")\nplt.show()\n",
      "outputs": []
    },
    {
      "id": "1f017f0c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\n# Language stats\ntry:\n    with open(LANGJSON, \"r\", encoding=\"utf-8\") as f:\n        lang_stats = json.load(f)[\"counts\"]\n    lang_stats\nexcept FileNotFoundError:\n    print(\"Language stats file not found; run the cleaning cell first.\")\n",
      "outputs": []
    }
  ]
}